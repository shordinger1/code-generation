import cv2
import time
import json
import requests
from collections import defaultdict
from typing import List, Dict
import base64
import re
import os
import threading
from threading import Thread
# from ultralytics import YOLOE
# import supervision as sv
import openai
from flask import Flask, request, jsonify, Response
import numpy as np
import logging
# from flask_cors import CORS
import queue
from openai import OpenAI
import shutil
import os
from concurrent.futures import ThreadPoolExecutor
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel

# ===========================
# æ¨¡æ‹Ÿå¤§æ¨¡å‹æ¥å£ï¼ˆè§†è§‰è¯†åˆ« + é—®ç­”ï¼‰
# å®é™…æƒ…å†µéœ€æ›¿æ¢ä¸ºä½ è‡ªå·±çš„å¤§æ¨¡å‹ API è°ƒç”¨
# ===========================
URL_7B = "http://36.156.143.242:8090/qwenvl2-7b"
URL_72B = "http://36.156.143.242:8090/describe-pic"
import concurrent.futures

MAX_PROCESSING_TIMEOUT = 10 * 60  # åˆ›å»ºä»»åŠ¡åï¼Œæœ€é•¿ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¦‚æœæœŸé—´æ²¡æœ‰ç­‰åˆ°åç»­è¯·æ±‚ï¼Œåˆ™ä»»åŠ¡è‡ªåŠ¨åˆ é™¤
openai.api_key = "sk-MLVRuDAU5EHZUG3U56A015Ef43A54d029e55295d8056FaEe"
client = openai.OpenAI(
    base_url="http://10.112.0.32:5239/v1",
    api_key="sk-MLVRuDAU5EHZUG3U56A015Ef43A54d029e55295d8056FaEe",
)
client_visual = openai.OpenAI(
    base_url="http://cc.komect.com/llm/vlgroup/",
    api_key="EMPTY",
)
# models = client_visual.models.list()
# model = models.data[0].id
processing_pool = dict()
thread_excutor = ThreadPoolExecutor(max_workers=8)
os.environ['PYTHONHASHSEED'] = '0'
app = Flask(__name__)


# CORS(app)  # å…è®¸æ‰€æœ‰åŸŸåè·¨åŸŸè¯·æ±‚ï¼ˆä¹Ÿå¯ä»¥è®¾ç½® originsï¼‰


def image_to_base64(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")


def chat_gpt(message):
    response = client.chat.completions.create(
        model="qwen2.5-7b-test",  # qwen72 æœ¬æ¬¡æ”¯æŒæ¨¡å‹
        stream=False,  # å¦‚æœéœ€è¦æµå¼å“åº”ï¼Œè¯·è®¾ç½®ä¸º True
        messages=message  # è¾“å…¥çš„ç”¨æˆ·æ¶ˆæ¯
    )

    # è¾“å‡ºæ¨¡å‹çš„éæµå¼å“åº”
    # for chunk in response._iter_events():
    #     content = chunk.data
    #     if content:
    #         print(content, end="\n", flush=True)
    return response


names = ['dining table', 'four-poster bed', 'Refrigerator', 'microwave oven', 'television', 'washer', 'cup',
         'paper knife', 'wooden spoon', 'flower',
         'remote control', 'comic book', 'mobile phone', 'hand-held computer', 'desktop computer', 'computer keyboard',
         'dishwasher', 'person', 'ruler', 'tandem bicycle']

# MODEL = YOLOE("pretrain/yoloe-v8l-seg.pt")
# MODEL.to("cuda:0")
# MODEL.set_classes(names, MODEL.get_text_pe(names))


thickness = 0


def extract_categories_from_question(class_dict: List[str], message: str) -> List[str]:
    formatted = [f"{i}-{item}" for i, item in enumerate(class_dict)]

    """
    ä»è‡ªç„¶è¯­è¨€é—®é¢˜ä¸­æå–å…³é”®è¯ç±»åˆ«
    """
    # æ„é€  prompt
    prompt = f""" ## Background ##
    ä½ æ˜¯ä¸€ä½è§†è§‰è¯­è¨€ç†è§£ä¸“å®¶ã€‚ç”¨æˆ·æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼Œä½ éœ€è¦æ ¹æ®è¯­ä¹‰ï¼Œåœ¨ä»¥ä¸‹ç±»ç›®ä¸­æ‰¾å‡ºæœ€ç›¸å…³çš„ä¸€é¡¹æˆ–å¤šé¡¹ã€‚

    å³ä½¿ç”¨æˆ·æ²¡æœ‰ç›´æ¥æåˆ°ç±»ç›®çš„åå­—ï¼Œä¹Ÿè¯·ç»“åˆå«ä¹‰åˆ¤æ–­æ˜¯å¦ç›¸å…³ã€‚

    è¯·ä»ç±»ç›®åˆ—è¡¨ä¸­è¿”å›æœ€ç›¸å…³çš„é¡¹ï¼ˆå¦‚â€æ°´æ¯â€œå’Œâ€æ‰‹æœºâ€œç­‰ï¼‰åœ¨åˆ—è¡¨ä¸­çš„ä½ç½®ï¼Œä¸å¿…è§£é‡ŠåŸå› ï¼Œä¸èƒ½è¿”å›ä¸åœ¨åˆ—è¡¨ä¸­çš„é¡¹ã€‚
    """
    query = message[-1]["content"]

    messages = [
        {'role': 'system', 'content': prompt},
        {
            "role": "user",
            "content":
                f"é—®é¢˜ï¼š{query},ä»æä¾›ç±»ç›®åˆ—è¡¨{formatted}ä¸­è¿”å›æœ€ç›¸å…³çš„é¡¹åœ¨åˆ—è¡¨ä¸­çš„ä½ç½®ï¼Œä¸å¿…è§£é‡ŠåŸå› ã€‚"
        }
    ]
    print(messages)

    # è°ƒç”¨å¤§æ¨¡å‹
    try:
        t1 = time.time()
        response = chat_gpt(messages)
        t2 = time.time()
        print(f"è¯­è¨€å¤§æ¨¡å‹è¯·æ±‚å®Œæˆï¼Œè€—æ—¶: {t2 - t1:.2f}ç§’")

        print(response.response)
        keywords = response.response.split(",")

        return keywords


    except Exception as e:
        print(f"APIè°ƒç”¨å¼‚å¸¸: {e}")
        return []


class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (np.integer, np.floating)):
            return obj.item()
        elif isinstance(obj, (np.str_, np.unicode_)):
            return str(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super().default(obj)


class QwenLLM(object):
    def __init__(
            self,
            url="http://10.112.0.32:5239/v1",
            backup_url="http://127.0.0.1:5020/v1/",
            # model_type="qwen14",
            model_type="qwen72",
            api_key="sk-tbXDmYh3m2VQgdA7931a909a19E24cBeAc6cC59783521969",
            max_tokens=8000, temperature=0.8, top_k=3
    ):
        self.url = url
        self.backup_url = backup_url
        self.api_key = api_key

        self.model_type = model_type
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.top_k = top_k

        self.llm_client = OpenAI(base_url=self.url, api_key=self.api_key)
        self.llm_client_bp = OpenAI(base_url=self.backup_url, api_key=self.api_key)

    def chat_stream(self, request_id, messages, do_search=False):
        start = time.time()

        try:
            response = self.llm_client.chat.completions.create(
                model=self.model_type,
                messages=messages,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                timeout=2,
                stream=True
            )
        except Exception as ee:
            print(str(ee))
            try:
                response = self.llm_client_bp.chat.completions.create(
                    model=self.model_type,
                    messages=messages,
                    max_tokens=self.max_tokens,
                    temperature=self.temperature,
                    timeout=2,
                    stream=True,
                    extra_body={"request_id": request_id}
                )
            except Exception as eee:
                print(str(eee))
                return None

        response.encoding = "utf-8"
        headers = {"Transfer-Encoding": "chunked", "Content-Type": "text/event-stream", "charset": "UTF-8"}
        output = self.stream_gen(response, do_search, start=start)
        # return Response(output, headers=headers, content_type="text/event-stream")
        return output

    def stream_gen(self, stream_resp, do_search, start):
        chunk_list = []
        for chunk in stream_resp._iter_events():
            try:
                answer_chunk = chunk.data
            except Exception:
                continue
            if not answer_chunk:
                continue
            if answer_chunk.strip() == "[DONE]":
                continue
            out_dict = {

                "choices": [
                    {
                        "index": 0,
                        "finish_reason": None,
                        "delta": answer_chunk
                    }
                ]

            }
            # output_str = json.dumps(out_dict, ensure_ascii=False)
            # self.logger.info(output_str.strip())
            chunk_list.append(answer_chunk)
            yield out_dict


class VisualLLM(object):
    def __init__(self, sn=None,
                 ):
        self.sn = sn
        if sn in processing_pool:
            self.processor = processing_pool[sn]
        else:
            logging.info(jsonify({'error': 'Stream not found in processing pool'}))

        result = self.processor.get_result()
        if result and 'class' in result and 'base64' in result:
            self.class_to_images = result['class']
            self.base64_to_id = result['base64']
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å›¾åƒæ•°æ®
            if not self.class_to_images or not self.base64_to_id:
                logging.info(jsonify({'error': 'No image data available for the stream'}))

    def chat_stream(self, request_id, messages, do_search=False):
        start = time.time()
        try:
            print("\nmessageï¼š", messages)
            # è·å–æ‰€æœ‰å¯ç”¨çš„ç±»åˆ«
            available_categories = list(self.class_to_images.keys())
            categories = extract_categories_from_question(available_categories, messages)
            print("è¯†åˆ«å‡ºçš„é—®é¢˜ç›¸å…³ç±»åˆ«ï¼š", categories)

            # æ”¶é›†æ‰€æœ‰ç›¸å…³ç±»åˆ«çš„å›¾ç‰‡ID
            all_image_ids = set()
            for category in categories:
                if category in self.class_to_images:
                    all_image_ids.update(self.class_to_images[category])
            all_image_ids = set()

            category = available_categories[int(categories[0])]
            if category in self.class_to_images:
                all_image_ids.update(self.class_to_images[category])
            # å»é‡åçš„å›¾ç‰‡IDåˆ—è¡¨
            unique_image_ids = list(all_image_ids)
            print(f"æ‰¾åˆ° {len(unique_image_ids)} å¼ ç›¸å…³å›¾ç‰‡")

            if unique_image_ids:
                # è·å–è¿™äº›å›¾åƒIDå¯¹åº”çš„base64ç¼–ç 
                unique_image_ids = unique_image_ids[-1:]
                images_base64 = [self.base64_to_id[img_id] for img_id in unique_image_ids]
                # å°†æ‰€æœ‰ç›¸å…³å›¾ç‰‡ä¸€èµ·å‘é€ç»™æ¨¡å‹
                output = self.stream_gen(request_id, ask_model_about_category(images_base64, messages, categories),
                                         do_search, start=start)
                return output
            else:
                print("\næœªæ‰¾åˆ°ç›¸å…³å›¾ç‰‡ï¼Œæ— æ³•å›ç­”é—®é¢˜ã€‚")
                return "æœªæ‰¾åˆ°ç›¸å…³å›¾ç‰‡ï¼Œæ— æ³•å›ç­”é—®é¢˜ã€‚"
        except Exception as ee:
            print(str(ee))
            return None

        # return Response(output, content_type="text/event-stream")

    def stream_gen(self, request_id, stream_resp, do_search, start):
        chunk_list = []
        for chunk in stream_resp:

            if chunk:
                # decoded_line = chunk.decode("utf-8")

                # # åˆ†å‰²å­—ç¬¦ä¸²ï¼Œæå–JSONéƒ¨åˆ†
                # decoded_line = decoded_line.split('data: ', 1)[1].strip()
                # # è§£æä¸ºå­—å…¸
                # decoded_line = json.loads( decoded_line)
                if chunk.choices[0].finish_reason == "stop":

                    resp = {
                        "choices": [
                            {
                                "index": 0,
                                "finish_reason": "stop",
                                "delta": chunk.choices[0].delta.content
                                # chunk["choices"][0]["message"]["content"]
                            }
                        ]

                    }
                    break
                else:
                    resp = {
                        "choices": [
                            {
                                "index": 0,
                                "finish_reason": "null",
                                "delta": chunk.choices[0].delta.content  # chunk["choices"][0]["message"]["content"]
                            }
                        ]

                    }

                # self.logger.info(output_str.strip()
                chunk_list.append(resp)
                yield resp
            print(chunk_list)


class StreamThread(Thread):
    def __init__(self, name, logger, llm, request_id, messages, do_search, output_queue):
        super().__init__(daemon=True)
        self.name = name
        self.logger = logger
        self.llm = llm
        self.request_id = request_id
        self.messages = messages
        self.do_search = do_search
        self.output_queue = output_queue

    def run(self, ):
        try:
            response = self.llm.chat_stream(
                request_id=self.request_id, messages=self.messages, do_search=self.do_search)
            for chunk in response:
                # å°†æµå¼è¾“å‡ºæ”¾å…¥é˜Ÿåˆ—
                self.output_queue.put((self.name, chunk))

            self.output_queue.put((self.name, None))  # æ ‡è®°æ­¤æ¥å£è¾“å‡ºç»“æŸ
        except Exception as e:
            print(e)
            self.output_queue.put((self.name, None))


class SearchLLM(object):
    def __init__(self, logger, foreword_llm, generator_llm):
        self.logger = logger
        self.foreword_llm = foreword_llm
        self.generator_llm = generator_llm

    def chat_stream(self, request_id, foreword_messages, generator_messages, do_search=False, server_start=False):
        def format_data(resp):
            return f"data:{json.dumps({'response': resp}, ensure_ascii=False)}\n\n"

        output_queue = queue.Queue(maxsize=0)
        # åˆ›å»ºå¼•å¯¼è¯­ç”Ÿæˆå¤§æ¨¡å‹çº¿ç¨‹
        foreword_thread = StreamThread(
            name="Foreword",
            logger=self.logger,
            llm=self.foreword_llm,
            request_id=request_id,
            messages=foreword_messages,
            do_search=do_search,
            output_queue=output_queue
        )
        # åˆ›å»ºè”ç½‘æœç´¢å¤§æ¨¡å‹çº¿ç¨‹
        generator_thread = StreamThread(
            name="Generator",
            logger=self.logger,
            llm=self.generator_llm,
            request_id=request_id,
            messages=generator_messages,
            do_search=do_search,
            output_queue=output_queue
        )
        generator_buffer = queue.Queue()  # ç”¨äºæš‚å­˜ã€è”ç½‘æœç´¢å¤§æ¨¡å‹ Generatorã€‘æµå¼è¾“å‡º

        # å¯åŠ¨çº¿ç¨‹
        foreword_thread.start()
        generator_thread.start()

        thread_name, thread_chunk = output_queue.get()
        # take_time = (time.time() - start) * 1000
        # self.logger.info("{} {:.2f}ms {}".format(thread_name, take_time, thread_chunk.strip()))

        first_thread_name = thread_name
        if first_thread_name == "Generator":
            # ã€è”ç½‘æœç´¢å¤§æ¨¡å‹Generatorã€‘å…ˆè¿”å›ï¼Œåˆ™ç›´æ¥è¾“å‡º
            while True:
                if thread_name == "Generator":
                    if thread_chunk is not None:
                        yield format_data(thread_chunk)
                    else:
                        break
                thread_name, thread_chunk = output_queue.get()
        else:  # ã€å¼•å¯¼è¯­ç”Ÿæˆå¤§æ¨¡å‹ Forewordã€‘å…ˆè¿”å›ï¼Œåˆ™å…ˆè¾“å‡ºå¼•å¯¼è¯­å†…å®¹ï¼Œå†è¾“å‡ºã€è”ç½‘æœç´¢å¤§æ¨¡å‹Generatorã€‘
            fore_finished_flag = False
            gen_finished_flag = False
            while True:
                if thread_name == "Foreword":
                    if thread_chunk is not None:
                        yield format_data(thread_chunk)
                    else:
                        fore_finished_flag = True
                elif thread_name == "Generator":
                    if thread_chunk is not None:
                        generator_buffer.put(thread_chunk)
                        if fore_finished_flag:
                            while not generator_buffer.empty():
                                buffer_chunk = generator_buffer.get()
                                yield format_data(buffer_chunk)
                    else:
                        gen_finished_flag = True

                if fore_finished_flag and gen_finished_flag:
                    break
                thread_name, thread_chunk = output_queue.get()

        while not generator_buffer.empty():
            buffer_chunk = generator_buffer.get()
            yield format_data(buffer_chunk)


def process_image(image):
    # results = MODEL.predict(image, verbose=False)
    # detections = sv.Detections.from_ultralytics(results[0])
    # if (len(detections.xyxy) == 0):
    #     return []
    # else:
    #     labels = [
    #         [0, 0, 0, 0, class_name, confidence]
    #         for class_name, confidence
    #         in zip(detections["class_name"], detections.confidence)]
    #     for i in range(len(detections.xyxy)):
    #         labels[i][0:4] = detections.xyxy[i][0:4]
    return [0, 0, 0, 0, 0, 0]


def process_grounding(frame):
    # start_time = time.time()
    result = process_image(frame)

    return [grounding for grounding in result if grounding[5] > thickness]


def generator_head(iter_response, sessionId):
    logging.info("è¿”å›è¯·æ±‚æ—¶é—´%s",
                 time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()) + f".{int(time.time() * 1000) % 1000:03d}")
    i = 0
    for line in iter_response.iter_lines():
        if line:
            try:
                decoded_line = line.decode("utf-8")

                # åˆ†å‰²å­—ç¬¦ä¸²ï¼Œæå–JSONéƒ¨åˆ†
                decoded_line = decoded_line.split('data: ', 1)[1].strip()
                # è§£æä¸ºå­—å…¸
                decoded_line = json.loads(decoded_line)
                if decoded_line["choices"][0]["finish_reason"] == "stop":

                    resp = {
                        "id": sessionId,
                        "choices": [{
                            "index": i,
                            "finish_reason": "stop",
                            "delta": decoded_line["choices"][0]["delta"]["content"]
                            # chunk["choices"][0]["message"]["content"]
                        }]
                    }
                    break
                else:
                    resp = {
                        "id": sessionId,
                        "choices": [{
                            "index": i,
                            "finish_reason": "null",
                            "delta": decoded_line["choices"][0]["delta"]["content"]
                            # chunk["choices"][0]["message"]["content"]

                        }]
                    }
                i = i + 1
                # yield f"data:{"response": {json.dumps(resp, ensure_ascii=False)}}\n\n"

                formatted_data = f"data:{json.dumps({'response': resp}, ensure_ascii=False)}\n\n"

                yield formatted_data
            except json.JSONDecodeError as e:
                print(f"Error decoding JSON: {e}")


def ask_model_about_category(images_64: List[str], question: str, category: str) -> str:
    """
    è°ƒç”¨å¤§æ¨¡å‹ï¼Œåˆ†ææŒ‡å®šå›¾åƒä¸­æŸç±»åˆ«çš„è¡Œä¸º
    """
    # æ„é€  prompt

    # EXTRACT_MEMORY_PROMPT = f"""
    #     ä½ æ˜¯ä¸€ä½è§†è§‰è®°å¿†ä¸æ¨ç†ä¸“å®¶ï¼Œæ‹¥æœ‰è¿ç»­å¤šå¸§å›¾åƒçš„è§‚å¯Ÿç»“æœã€‚

    #     è¯·ä½ æ ¹æ®ä»¥ä¸‹è¿ç»­æ‹æ‘„çš„å›¾åƒï¼Œåˆ†æç”¨æˆ·çš„é—®é¢˜ï¼Œå¹¶æ ¹æ®å›¾åƒä¸­çš„å†…å®¹è¿›è¡Œæ¨ç†åˆ¤æ–­ã€‚
    #     ç”¨æˆ·é—®é¢˜ï¼š
    #     {question}

    #     è¯·ä½ åˆ¤æ–­é—®é¢˜ä¸­çš„ç‰©ä½“åœ¨å›¾åƒä¸­çš„è¡¨ç°ã€å˜åŒ–æˆ–ä½ç½®ï¼Œå¹¶ç»™å‡ºç›´æ¥æ˜ç¡®çš„ç­”æ¡ˆã€‚

    #     å¦‚æœæ¶‰åŠç©ºé—´ä½ç½®ï¼Œè¯·åªè¿”å›ç‰©ä½“å½“å‰æ‰€åœ¨çš„ä½ç½®æè¿°ï¼ˆå¦‚"åœ°ä¸Š"ã€"æ¡Œå­ä¸‹"ç­‰ï¼‰ï¼Œä¸è¦è§£é‡Šè¿‡ç¨‹æˆ–ç¼–å·ã€‚"""

    # messages = [
    #      {'role': 'system', 'content': SYS_MEMORY_PROMPT},
    #      {
    #         "role": "user",
    #         "content": [
    #             {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{images_64}"}},
    #             {"type": "text", "text":question},
    #         ],
    #      }
    #     ]

    query = question[-1]["content"]

    # messages[-1]['content'].append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{images_64}"}})

    #     # æ‰¾åˆ° system éƒ¨åˆ†å¹¶æ‹¼æ¥
    # for message in messages:
    #     if message["role"] == "system":
    #         message["content"] = SYS_MEMORY_PROMPT
    role_name1 = "æ°´æ¯"
    img_str1 = image_to_base64("prompt_picture/cup1.jpg")
    img_str2 = image_to_base64("prompt_picture/cup2.jpg")

    SYS_MEMORY_PROMPT = """ ## Background ##
    ä½ å…·æœ‰é«˜çº§å›¾åƒåˆ†æç³»ç»Ÿï¼Œå·²çŸ¥{role_name1}åœ¨å›¾åƒä¸­ï¼Œè¯·æ ¹æ®è¾“å…¥çš„å›¾åƒï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚ä¸è¦å›ç­”ä»»ä½•æ— å…³çš„é—®é¢˜ï¼Œç¦æ­¢å‡ºç°ç¤ºä¾‹çš„å›ç­”,ç¦æ­¢åœ¨å›ç­”ä¸­å‡ºç°"å›¾ç‰‡"å’Œ"å›¾åƒ"ã€‚
# """
    content_role0 = [
        {"type": "text", "text": SYS_MEMORY_PROMPT},
    ]
    content_role1 = [
        {"type": "image_url", "image_url": {"url": f"data:image;base64,{img_str1}"}},
        {"type": "text", "text": f"æ³¨æ„ï¼ä»¥ä¸Šè¿™ä¸ªå›¾ç‰‡ä¸­é»„è‰²æ¡†å‡ºæ¥çš„å±•ç°çš„æ˜¯{role_name1}"},
        {"type": "text", "text": "æ³¨æ„!!!ï¼è¿™æ˜¯ä¸€ä¸ªå¯ä¾›å­¦ä¹ çš„ç¤ºä¾‹ï¼Œè¯·å­¦ä¹ è¿™ä¸ªç¤ºä¾‹ä½œä¸ºæ ‡å‡†çŸ¥è¯†ï¼Œå¹¶ä½œä¸ºå›ç­”ä»¥ä¸‹é—®é¢˜çš„èƒŒæ™¯"}]
    content_role2 = [
        {"type": "image_url", "image_url": {"url": f"data:image;base64,{img_str2}"}},
        {"type": "text", "text": f"æ³¨æ„ï¼ä»¥ä¸Šè¿™ä¸ªå›¾ç‰‡ä¸­é»„è‰²æ¡†å‡ºæ¥çš„å±•ç°çš„æ˜¯{role_name1}"},
        {"type": "text", "text": "æ³¨æ„!!!ï¼è¿™æ˜¯ä¸€ä¸ªå¯ä¾›å­¦ä¹ çš„ç¤ºä¾‹ï¼Œè¯·å­¦ä¹ è¿™ä¸ªç¤ºä¾‹ä½œä¸ºæ ‡å‡†çŸ¥è¯†ï¼Œå¹¶ä½œä¸ºå›ç­”ä»¥ä¸‹é—®é¢˜çš„èƒŒæ™¯"}]
    sys_case = {
        "role": "system",
        "content": content_role0 + content_role1 + content_role2
    }

    messages = [
        #  {'role': 'system', 'content': SYS_MEMORY_PROMPT},
        sys_case,
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{images_64}"}},
                {"type": "text", "text": query},
            ],
        }
    ]

    try:
        t1 = time.time()
        # prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è®¡ç®—æœºè§†è§‰ä¸“å®¶ï¼Œæ“…é•¿ç›®æ ‡æ£€æµ‹å’Œç‰©ä½“è¯†åˆ«ã€‚è¯·å¯¹æä¾›çš„å›¾åƒè¿›è¡Œå…¨é¢çš„ç›®æ ‡æ£€æµ‹ï¼Œè¯†åˆ«å‡ºå›¾åƒä¸­çš„æ‰€æœ‰ç‰©ä½“ã€‚åªè¿”å›ç±»åˆ«åç§°åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”ã€‚
        # è¾“å‡ºç±»åˆ«ä»…é™äº "æ‰‹æœº,æ¡Œå­,æ‰‹æœº,ç”µè„‘,ç¬”,æ°´æ¯,åœ°æ¿,æ¤…å­,èŠ±"è¿™ä¹ç§ç±»åˆ«ä¸­çš„ä¸€ç§ã€‚"""
        # messages_detection = [
        #     {'role': 'system', 'content': prompt},
        #     {
        #         "role": "user",
        #         "content": [
        #             {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}},
        #             {"type": "text", "text": f"è¯·è¯†åˆ«å›¾åƒä¸­çš„æ‰€æœ‰ç‰©ä½“ç±»åˆ«ï¼Œåªè¿”å›ç±»åˆ«åç§°åˆ—è¡¨ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”ã€‚"},
        #         ],
        #     }
        #     ]
        response = client_visual.chat.completions.create(
            model='Qwen2.5-VL-72B-Instruct-AWQ',
            messages=messages,
            max_tokens=64,
            stream=True
        )
        # print(response.choice[0].message.content)
        t2 = time.time()

        print(f"è§†è§‰å¤§æ¨¡å‹è¯·æ±‚å®Œæˆï¼Œè€—æ—¶: {t2 - t1:.2f}ç§’")

        # éæµå¼è¿”å›
        # result = r
        # sponse.json()
        # # è§£æè¿”å›çš„JSONï¼Œæå–ç±»åˆ«
        # print(f"âœ… è¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
        # decoded_line = response.content.decode("utf-8")
        # raw_str = json.loads(decoded_line)["response"]["choices"][0]["message"]["content"]
        # # æ¸…æ´—æ‰ markdown çš„ ```json å’Œ ``` æ ‡è®°
        # return raw_str
        ##æµå¼è¿”å›
        return response
        # return Response(generator_head(response,session_id), content_type='text/event-stream')


    except Exception as e:
        print(f"APIè°ƒç”¨å¼‚å¸¸: {e}")
        return "æ— æ³•ç¡®å®šä½ç½®"


# ä¿®æ”¹ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°ï¼Œä½¿å…¶é€‚ç”¨äºå•å¸§å¤„ç†
def compute_frame_similarity(current_frame, last_frame_feature, processor, model, device):
    """
    è®¡ç®—å½“å‰å¸§ä¸ä¸Šä¸€ä¸ªä¿ç•™å¸§ä¹‹é—´çš„ç›¸ä¼¼åº¦

    å‚æ•°:
        current_frame: å½“å‰å¸§å›¾åƒ
        last_frame_feature: ä¸Šä¸€ä¸ªä¿ç•™å¸§çš„ç‰¹å¾
        processor: CLIPå¤„ç†å™¨
        model: CLIPæ¨¡å‹
        device: è®¡ç®—è®¾å¤‡

    è¿”å›:
        ç›¸ä¼¼åº¦å€¼ï¼Œä»¥åŠå½“å‰å¸§çš„ç‰¹å¾
    """
    # è½¬æ¢ä¸ºPILå›¾åƒ
    pil_image = Image.fromarray(cv2.cvtColor(current_frame, cv2.COLOR_BGR2RGB))
    inputs = processor(images=pil_image, return_tensors="pt").to(device)

    with torch.no_grad():
        features = model.get_image_features(**inputs)

    # å½’ä¸€åŒ–ç‰¹å¾
    features = features / features.norm(dim=-1, keepdim=True)
    last_frame_feature = last_frame_feature.to(device)

    # è®¡ç®—ç›¸ä¼¼åº¦
    similarity = torch.nn.functional.cosine_similarity(
        last_frame_feature, features
    ).item()

    return similarity, features.cpu()


class processing_backend:
    def __init__(self, sn: int):
        self.sn_str = str(sn)
        self.fresh_time = time.time()
        self.grouding = []
        self.should_stop = False
        self.cached_frame = dict()
        self.request_interval = 1
        self.process_video_thread = None
        # self.processsing_vedio_backend(self.stream_url)
        # self.start_processing()
        self.executor = ThreadPoolExecutor(max_workers=10)  # è¿™é‡Œåˆ›å»ºå›ºå®š1ä¸ªçº¿ç¨‹çš„æ± 

        self.process_video_future = None

    def processsing_vedio_backend(self, stream_url: str):
        """
        ä½¿ç”¨YOLOv8è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œæ¯ç§’å¤„ç†ä¸€å¸§å›¾åƒ
        """

        try:
            folder = f"run/{self.sn_str}"
            if os.path.exists(folder):
                shutil.rmtree(folder)
                print("æ–‡ä»¶å¤¹å·²åˆ é™¤")
            else:
                print("æ–‡ä»¶å¤¹ä¸å­˜åœ¨")

            frame_dir = f"run/{self.sn_str}/frames"
            cls_dir = f"run/{self.sn_str}/cls"
            if not os.path.exists(frame_dir):
                os.makedirs(frame_dir)
            if not os.path.exists(cls_dir):
                os.makedirs(cls_dir)

            # è·å–åŸå§‹è§†é¢‘çš„å¸§ç‡
            cap = cv2.VideoCapture(stream_url)
            if not cap.isOpened():
                raise Exception("Failed to open video stream")

            original_fps = cap.get(cv2.CAP_PROP_FPS)

            frame_id = 0
            processed_frame_id = 0
            last_request_time = 0  ##è°ƒè¿‡ç¬¬ä¸€å¸§ï¼Œå€¼ä¸ºtime.time()
            MAX_FRAME_TIMEOUT = 5  # 5 seconds timeout for frame processing
            # åˆå§‹åŒ–ç›¸ä¼¼åº¦è®¡ç®—å˜é‡
            last_frame = None
            last_frame_feature = None
            similarity_threshold = 0.98

            # åŠ è½½CLIPæ¨¡å‹ï¼ˆé¢„å…ˆåŠ è½½ä»¥é¿å…é‡å¤åŠ è½½ï¼‰
            print("åŠ è½½CLIPæ¨¡å‹...")
            t1 = time.time()
            model = CLIPModel.from_pretrained("/data/sjc/models/clip-vit-large-patch14")
            processor = CLIPProcessor.from_pretrained("/data/sjc/models/clip-vit-large-patch14")
            device = "cuda" if torch.cuda.is_available() else "cpu"
            model = model.to(device)
            model.eval()
            print(f"åŠ è½½å®Œæ¯•ï¼Œè€—æ—¶ï¼š{time.time() - t1}")
            first_request_time = time.time()
            while not self.should_stop:
                try:
                    # è¯»å–å¸§
                    ret, frame = cap.read()

                    if not ret:
                        MAX_RETRY = 3
                        retry = 0
                        while retry < MAX_RETRY and not cap.isOpened():
                            cap.release()
                            cap = cv2.VideoCapture(stream_url)
                            retry += 1
                            time.sleep(1)  # Wait before retry
                        ret, frame = cap.read()
                        if not cap.isOpened() or not ret:
                            raise Exception("Failed to read video frame after retries")

                    # æ ¹æ®æ—¶é—´è·³è¿‡å¸§
                    current_time = time.time()

                    if current_time - last_request_time < self.request_interval:
                        continue
                    last_request_time = current_time
                    start_time = time.time()
                    image_id = f"img_{processed_frame_id:04d}"

                    # å¤„ç†ç¬¬ä¸€å¸§
                    if last_frame is None:
                        last_frame = frame.copy()
                        # ä¸ºç¬¬ä¸€å¸§æå–ç‰¹å¾
                        pil_image = Image.fromarray(cv2.cvtColor(last_frame, cv2.COLOR_BGR2RGB))
                        inputs = processor(images=pil_image, return_tensors="pt").to(device)

                        with torch.no_grad():
                            last_frame_feature = model.get_image_features(**inputs)

                        # å½’ä¸€åŒ–ç‰¹å¾
                        last_frame_feature = last_frame_feature / last_frame_feature.norm(dim=-1, keepdim=True)

                        # å¯åŠ¨çº¿ç¨‹ç”¨å¤§æ¨¡å‹è¿›è¡Œç›®æ ‡æ£€æµ‹
                        detection_thread = threading.Thread(
                            target=self.vlm_detetion,
                            args=(frame, image_id, frame_dir),
                            daemon=True
                        )
                        detection_thread.start()
                        print(f"ç¬¬{current_time - first_request_time}ç§’ï¼Œå¤„ç†ç¬¬ä¸€å¸§: {image_id}")
                        # Wait for detection thread with timeout
                        # detection_thread.join(timeout=MAX_FRAME_TIMEOUT)
                        # if detection_thread.is_alive():
                        #     print(f"Warning: Detection for frame {image_id} timed out")

                        processed_frame_id += 1
                    else:
                        # è®¡ç®—å½“å‰å¸§ä¸ä¸Šä¸€ä¸ªä¿ç•™å¸§çš„ç›¸ä¼¼åº¦
                        similarity, current_feature = compute_frame_similarity(
                            frame, last_frame_feature, processor, model, device
                        )

                        # å¦‚æœç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ï¼Œåˆ™å¤„ç†å½“å‰å¸§
                        if similarity < similarity_threshold:
                            image_id = f"img_{processed_frame_id:04d}"
                            detection_thread = threading.Thread(target=self.vlm_detetion,
                                                                args=(frame.copy(), image_id, frame_dir),
                                                                daemon=True)
                            detection_thread.start()
                            print(
                                f"ç¬¬{current_time - first_request_time}ç§’ï¼Œå¤„ç†ç¬¬ {image_id}å¸§:, ç›¸ä¼¼åº¦: {similarity:.4f}")
                            # Wait for detection thread with timeout
                            # detection_thread.join(timeout=MAX_FRAME_TIMEOUT)
                            # if detection_thread.is_alive():
                            #     print(f"Warning: Detection for frame {image_id} timed out")

                            # æ›´æ–°å‚è€ƒå¸§å’Œç‰¹å¾
                            last_frame = frame.copy()
                            last_frame_feature = current_feature
                            processed_frame_id += 1
                        else:
                            print(
                                f"ç¬¬{current_time - first_request_time}ç§’,è·³è¿‡{image_id}ç›¸ä¼¼å¸§:, ç›¸ä¼¼åº¦: {similarity:.4f}")
                            # print(f"è·³è¿‡ç›¸ä¼¼å¸§ï¼Œç›¸ä¼¼åº¦: {similarity:.4f}",end = "")
                            # è®¡ç®—ä¸‹ä¸€å¸§çš„æ—¶é—´ç‚¹ï¼Œç¡®ä¿æ¯ç§’å¤„ç†ä¸€å¸§
                            elapsed_time = time.time() - start_time
                            if elapsed_time < self.request_interval:
                                time.sleep(self.request_interval - elapsed_time)
                except Exception as e:
                    print(f"Error processing frame: {str(e)}")
                    time.sleep(1)  # Wait before retrying

        except Exception as e:
            print(f"Fatal error in video processing: {str(e)}")
        finally:
            if 'cap' in locals():
                cap.release()
            self.should_stop = True  # Ensure processing stops on error

    def get_latest_frame_info(self):
        """
        è·å–æœ€æ–°å¤„ç†çš„ä¸€å¼ å›¾åƒå’Œå¯¹åº”çš„ç‰©ä½“ç±»åˆ«
        è¿”å›æ ¼å¼ï¼š
        {
            "frame_id": "img_0001",
            "image_base64": "xxx...",
            "categories": ["æ‰‹æœº", "æ°´æ¯", "æ¡Œå­"]
        }
        """
        if not self.cached_frame:
            return None

        # å–æœ€æ–°ä¸€å¼ 
        latest_frame_id = sorted(self.cached_frame.keys())[-1]  # æŒ‰ frame_id æ’åºå–æœ€åä¸€ä¸ª

        image_base64 = self.cached_frame[latest_frame_id]

        # æŸ¥æ‰¾è¿™ä¸ª frame_id å¯¹åº”çš„ç±»åˆ«
        detected_classes = []
        class_to_images = self.get_class_to_images()
        for cls, image_ids in class_to_images.items():
            if str(latest_frame_id) in image_ids:
                detected_classes.append(cls)

        return {
            "frame_id": latest_frame_id,
            "image_base64": image_base64,
            "categories": detected_classes
        }

    def get_class_to_images(self):
        self.waiting_time = 0
        root_dir = f"run/{self.sn_str}/cls"
        class_to_images = {}
        for filename in os.listdir(root_dir):
            # æ‹¼æ¥å®Œæ•´æ–‡ä»¶è·¯å¾„
            file_path = os.path.join(root_dir, filename)
            # ç¡®ä¿å¤„ç†çš„æ˜¯æ–‡ä»¶ï¼ˆè€Œéå­ç›®å½•ï¼‰
            if os.path.isfile(file_path):
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(file_path, 'r', encoding='utf-8') as file:
                    # ä½¿ç”¨splitlines()è‡ªåŠ¨å»é™¤æ¢è¡Œç¬¦
                    lines = file.read().splitlines()

                # å°†ç»“æœå­˜å…¥å­—å…¸
                class_to_images[filename.replace("_to_images.json", "")] = lines
        return class_to_images

    def get_base64_to_id(self):
        return self.cached_frame
        # self.waiting_time = 0
        # while True:
        #     try:
        #         with open(f"run/{self.sn_str}/base64_to_id.json", "r", encoding="utf-8") as f:
        #             datas= f.read().splitlines()
        #             result = dict(map(lambda s: list(json.loads(s).items())[0], datas))

        #             return  result #[json.loads(data) for data in datas]
        #     except Exception as e:
        #         print(f"Error reading base64_to_id.json: {e}")
        #         continue

    def get_grounding(self):
        return self.grouding

    def get_result(self):
        self.fresh_time = time.time()
        return {"class": self.get_class_to_images(), "base64": self.get_base64_to_id()}

    def start_processing(self, stream_url):
        if self.process_video_future is None or self.process_video_future.done():
            self.process_video_future = self.executor.submit(
                self.processsing_vedio_backend, stream_url
            )
            self.should_stop = False
            self.fresh_time = time.time()
        return {'status': 'processing'}

    def stop_processing(self):
        self.should_stop = True
        if self.process_video_future:
            try:
                self.process_video_future.cancel()
            except Exception as e:
                print(f"å–æ¶ˆçº¿ç¨‹å‡ºé”™: {e}")
        return {'status': 'stopping'}

    def vlm_detetion(self, frame, frame_id, frame_dir):
        _, buffer = cv2.imencode('.jpg', frame)
        image_base64 = base64.b64encode(buffer).decode('utf-8')

        role_name1 = "æ°´æ¯"
        img_str1 = image_to_base64("prompt_picture/cup4.jpg")
        img_str2 = image_to_base64("prompt_picture/cup2.jpg")

        SYS_MEMORY_PROMPT = """ ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è®¡ç®—æœºè§†è§‰ä¸“å®¶ï¼Œæ“…é•¿ç›®æ ‡æ£€æµ‹å’Œç‰©ä½“è¯†åˆ«ã€‚è¯·å¯¹æä¾›çš„å›¾åƒè¿›è¡Œå…¨é¢çš„ç›®æ ‡æ£€æµ‹ï¼Œè¯†åˆ«å‡ºå›¾åƒä¸­çš„æ‰€æœ‰ç‰©ä½“ã€‚åªè¿”å›ç±»åˆ«åç§°åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”ã€‚   
        è¾“å‡ºç±»åˆ«ä»…é™äº "æ‰‹æœº,æ¡Œå­,æ‰‹æœº,ç”µè„‘,ç¬”,æ°´æ¯,åœ°æ¿,æ¤…å­,èŠ±"è¿™ä¹ç§ç±»åˆ«ä¸­çš„ä¸€ç§,ä¸å…è®¸å‡ºç°ä¸åœ¨è¿™ä¹ç§çš„ç±»åˆ«ã€‚"""

        content_role0 = [
            {"type": "text", "text": SYS_MEMORY_PROMPT},
        ]
        content_role1 = [
            {"type": "image_url", "image_url": {"url": f"data:image;base64,{img_str1}"}},
            {"type": "text", "text": f"æ³¨æ„ï¼ä»¥ä¸Šè¿™ä¸ªå›¾ç‰‡ä¸­é»„è‰²æ¡†å‡ºæ¥çš„å±•ç°çš„æ˜¯{role_name1}"},
            {"type": "text",
             "text": "æ³¨æ„!!!ï¼è¿™æ˜¯ä¸€ä¸ªå¯ä¾›å­¦ä¹ çš„ç¤ºä¾‹ï¼Œè¯·å­¦ä¹ è¿™ä¸ªç¤ºä¾‹ä½œä¸ºæ ‡å‡†çŸ¥è¯†ï¼Œå¹¶ä½œä¸ºå›ç­”ä»¥ä¸‹é—®é¢˜çš„èƒŒæ™¯"}]
        content_role2 = [
            {"type": "image_url", "image_url": {"url": f"data:image;base64,{img_str2}"}},
            {"type": "text", "text": f"æ³¨æ„ï¼ä»¥ä¸Šè¿™ä¸ªå›¾ç‰‡ä¸­é»„è‰²æ¡†å‡ºæ¥çš„å±•ç°çš„æ˜¯{role_name1}"},
            {"type": "text",
             "text": "æ³¨æ„!!!ï¼è¿™æ˜¯ä¸€ä¸ªå¯ä¾›å­¦ä¹ çš„ç¤ºä¾‹ï¼Œè¯·å­¦ä¹ è¿™ä¸ªç¤ºä¾‹ä½œä¸ºæ ‡å‡†çŸ¥è¯†ï¼Œå¹¶ä½œä¸ºå›ç­”ä»¥ä¸‹é—®é¢˜çš„èƒŒæ™¯"}]
        sys_case = {
            "role": "system",
            "content": content_role0 + content_role1 + content_role2
        }
        messages_detection = [
            #  {'role': 'system', 'content': SYS_MEMORY_PROMPT},
            sys_case,
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}},
                    {"type": "text", "text": f"è¯·è¯†åˆ«å›¾åƒä¸­çš„æ‰€æœ‰ç‰©ä½“ç±»åˆ«ï¼Œåªè¿”å›ç±»åˆ«åç§°åˆ—è¡¨ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”ã€‚"},
                ],
            }
        ]
        try:
            t1 = time.time()
            response = client_visual.chat.completions.create(
                model='Qwen2.5-VL-72B-Instruct-AWQ',
                messages=messages_detection,
                max_tokens=64,
                timeout=30
            )
            t2 = time.time()
            print(f"å¤§æ¨¡å‹å¤„ç†{frame_id},ç”¨æ—¶{t2 - t1}")
            # è§£æä¸ºå­—å…¸
            raw_data = response.choices[0].message.content.strip("`").strip("json").strip()
            results = raw_data.split(",")
            # payload = {
            #     "stream": False,
            #     "messages": messages_detection,
            #     "max_tokens": 1024
            # }

            # headers = {"Content-Type": "application/json"}
            # t1 = time.time()

            # response = requests.post(URL_72B, json=payload, headers=headers, timeout=100, stream=False)
            # t2 = time.time()
            # print(f"å¤§æ¨¡å‹è¯·æ±‚è€—æ—¶:{t2-t1}")
            # decoded_line = response.content.decode("utf-8")

            # raw_str = json.loads( decoded_line)["response"]["choices"][0]["message"]["content"]
            # # è§£æä¸ºæ•°ç»„
            # results = raw_str.split(",")

            categories = []
            for cls_name in results:
                categories.append(cls_name)

            # ä¿å­˜å¸§åˆ°æœ¬åœ°frameæ–‡ä»¶å¤¹
            frame_path = os.path.join(frame_dir, f"{frame_id}.jpg")
            cv2.imwrite(frame_path, frame)
            print(f"ä¿å­˜å¸§åˆ°: {frame_path}")

            # åªæœ‰å½“æ£€æµ‹åˆ°ç±»åˆ«æ—¶æ‰ä¿å­˜å›¾åƒ
            if categories and categories[0]:
                # å°†å›¾åƒè½¬æ¢ä¸ºbase64
                # resized_frame = np.resize(frame, (720, 1280, 3))
                _, buffer = cv2.imencode('.jpg', frame)
                image_base64 = base64.b64encode(buffer).decode('utf-8')
                for cls in categories:
                    while True:
                        try:
                            with open(f"run/{self.sn_str}/cls/{cls}_to_images.json", "a", encoding="utf-8") as f:
                                f.write(str(frame_id) + '\n')
                                break
                        except:
                            print("å†™å…¥class_to_imagesæ–‡ä»¶é”™è¯¯")

                self.cached_frame[frame_id] = image_base64

                print(f"[{frame_id}] è¯†åˆ«åˆ°ç±»åˆ«: {categories}")
            else:
                print(f"[{frame_id}] æœªæ£€æµ‹åˆ°ä»»ä½•ç±»åˆ«ï¼Œè·³è¿‡ä¿å­˜")
        except Exception as e:
            ##å¤‡ç”¨æ¨¡å‹
            print(e)


def process_question(class_to_images: Dict[str, List[str]], base64_to_id: Dict[str, str], message: str,
                     session_id: int):
    print("\nmessageï¼š", message)
    # è·å–æ‰€æœ‰å¯ç”¨çš„ç±»åˆ«
    available_categories = list(class_to_images.keys())
    categories = extract_categories_from_question(available_categories, message)
    print("è¯†åˆ«å‡ºçš„é—®é¢˜ç›¸å…³ç±»åˆ«ï¼š", categories)

    # æ”¶é›†æ‰€æœ‰ç›¸å…³ç±»åˆ«çš„å›¾ç‰‡ID
    all_image_ids = set()
    for category in categories:
        if category in class_to_images:
            all_image_ids.update(class_to_images[category])

    # å»é‡åçš„å›¾ç‰‡IDåˆ—è¡¨
    unique_image_ids = list(all_image_ids)
    print(f"æ‰¾åˆ° {len(unique_image_ids)} å¼ ç›¸å…³å›¾ç‰‡")

    if unique_image_ids:
        # è·å–è¿™äº›å›¾åƒIDå¯¹åº”çš„base64ç¼–ç 
        unique_image_ids = unique_image_ids[-2:]
        images_base64 = [base64_to_id[img_id] for img_id in unique_image_ids]
        # å°†æ‰€æœ‰ç›¸å…³å›¾ç‰‡ä¸€èµ·å‘é€ç»™æ¨¡å‹
        return ask_model_about_category(images_base64, message, categories)
    else:
        print("\næœªæ‰¾åˆ°ç›¸å…³å›¾ç‰‡ï¼Œæ— æ³•å›ç­”é—®é¢˜ã€‚")
        return "æœªæ‰¾åˆ°ç›¸å…³å›¾ç‰‡ï¼Œæ— æ³•å›ç­”é—®é¢˜ã€‚"


@app.route('/process_video', methods=['POST'])
def process_video():
    data = request.json
    print(data)

    stream_url = data.get('stream_url')
    flag = data.get('flag')
    sn = data.get('sn')

    def generate():
        try:
            if sn in processing_pool:
                backend = processing_pool[sn]
            else:
                backend = processing_backend(sn)
                processing_pool[sn] = backend

            backend.start_processing(stream_url)
            start_time = time.time()

            last_sent_frame_id = None  # ğŸ”¥ ä¸Šä¸€æ¬¡å‘é€çš„ frame_id

            def should_terminate(backend, start_time):
                return (
                        (time.time() - start_time > MAX_PROCESSING_TIMEOUT) or
                        backend.should_stop
                )

            while not should_terminate(backend, start_time):
                frame_info = backend.get_latest_frame_info()

                if frame_info:
                    if frame_info["frame_id"] != last_sent_frame_id:
                        # æœ‰æ–°å¸§ï¼Œæ¨é€
                        payload = {
                            "frame_id": frame_info["frame_id"],
                            "categories": frame_info["categories"],
                            "image_base64": frame_info["image_base64"]
                        }
                        yield f"data: {json.dumps(payload, ensure_ascii=False)}\n\n"

                        last_sent_frame_id = frame_info["frame_id"]  # æ›´æ–°å‘é€è®°å½•
                    else:
                        # æ²¡æœ‰æ–°å¸§ï¼Œä¸æ¨é€
                        pass

                # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡ï¼Œå› ä¸º1ç§’æ‰å‡ºæ–°å¸§
                time.sleep(1)

        except GeneratorExit:
            print(f"Client disconnected for sn: {sn}")
        except Exception as e:
            print(f"Error in SSE connection: {e}")

    def start():
        try:
            return Response(generate(), content_type='text/event-stream')  # Newline Delimited JSON
        except Exception as e:
            print(f"Start Error: {str(e)}")
            return jsonify({'flag': 'fail', 'message': str(e)}), 500

    def stop(sn):
        if sn in processing_pool:
            backend = processing_pool[sn]
            result = backend.stop_processing()
            time.sleep(0.1)
            return jsonify({'flag': 'success', 'message': 'Stopped processing', **result}), 200
        else:
            return jsonify({'flag': 'fail', 'message': 'Stream not found'}), 404

    if flag == "start":
        return start()
    if flag == "stop":
        return stop(sn)
    return jsonify({'status': 'error', 'message': 'æ²¡æœ‰æ ‡å¿—ä½'})


@app.route('/memory_vqa', methods=['POST'])
def memory_vqa():
    data = request.json
    sn = data.get('sn')

    message = data.get('message')
    session_id = data.get('session_id')
    request_id = data.get('request_id')
    logging.info(data)

    if not sn or not message:
        return jsonify({'flag': 'fail', 'message': 'Missing sn or message parameter'}), 400

    try:
        # æ£€æŸ¥snæ˜¯å¦åœ¨processing_poolä¸­
        if sn not in processing_pool:
            # å¦‚æœæ²¡æœ‰è¿™ä¸ªæµï¼Œç›´æ¥è¿”å›é»˜è®¤å›å¤

            local_llm = QwenLLM()
            query = "æˆ‘æ²¡æœ‰çœ‹åˆ°ç”»é¢"
            messages_local = [
                {'role': 'system', 'content': """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯­è¨€åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·è¦æ±‚é‡å¤ç”¨æˆ·çš„é—®é¢˜ã€‚
                        ä¸è¦è§£é‡Šä½ çš„åˆ†æè¿‡ç¨‹ï¼Œç›´æ¥é‡å¤é—®é¢˜å³å¯ï¼Œå›å¤å­—æ•°ä¸€æ ·ã€‚"""},
                {'role': 'user', 'content': query}
            ]

            def format_data(resp):
                for chunk in resp:
                    # å°†æµå¼è¾“å‡ºæ”¾å…¥é˜Ÿåˆ—

                    yield f"data:{json.dumps({'response': chunk}, ensure_ascii=False)}\n\n"

            return Response(format_data(local_llm.chat_stream(request_id=request_id, messages=messages_local)),
                            content_type='text/event-stream')

        local_llm = QwenLLM()
        visual_llm = VisualLLM(sn=sn)
        output_llm = SearchLLM(session_id, local_llm, visual_llm)
        query = message[-1]["content"]
        messages_local = [
            {'role': 'system', 'content': """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†è§‰é—®ç­”åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„å«è¯ï¼Œä¸ºåç»­çš„è§†è§‰åˆ†æç»“æœåšé“ºå«ã€‚
                        å«è¯åº”è¯¥ç®€æ´æ˜äº†ï¼Œç›´æ¥ç‚¹æ˜ç”¨æˆ·çš„é—®é¢˜ï¼Œå¹¶å¼•å¯¼ç”¨æˆ·å…³æ³¨å³å°†åˆ°æ¥çš„è§†è§‰åˆ†æç»“æœã€‚
                        ä¸è¦è§£é‡Šä½ çš„åˆ†æè¿‡ç¨‹ï¼Œç›´æ¥ç»™å‡ºå«è¯å³å¯ï¼Œä¸è¶…è¿‡10ä¸ªå­—ã€‚"""},
            {'role': 'user', 'content': query}
        ]

        return Response(output_llm.chat_stream(request_id, messages_local, message), content_type='text/event-stream')

    except Exception as e:
        print(f"memory_vqa Error: {str(e)}")
        return jsonify({'flag': 'fail', 'message': str(e)}), 500


def thread_manage():
    count = 1
    while True:
        count += 1
        if count % 10 == 0:
            print(f"thread counting:{len(threading.enumerate())}")
        time.sleep(1)
        for k, v in processing_pool.items():
            time.sleep(1)
            if isinstance(v, processing_backend):
                # print(f"sn{v.sn_str}:last process time:{time.time() -v.fresh_time}")
                if time.time() - v.fresh_time > MAX_PROCESSING_TIMEOUT:
                    print(f"\n\n\n\n\nsn{v.sn_str}:stopped after {time.time() - v.fresh_time}\n\n\n\n\n")
                    v.should_stop = True
                    time.sleep(1)
                    processing_pool.pop(k)
                    # del v
                    break


if __name__ == "__main__":
    check_thread = threading.Thread(target=thread_manage, daemon=True)
    check_thread.start()
    flag = False
    if flag == True:
        app.run(host='0.0.0.0', port=9005, threaded=True)
    else:
        app.run(host='0.0.0.0', port=9006, threaded=True)

    # sn = 1
    # backend1 = processing_backend(sn, 0)
    # processing_pool[sn]=backend1
    # sn+=1
    # backend2 = processing_backend(sn, 0)
    # processing_pool[sn]=backend2
    # sn+=1
    # backend3 = processing_backend(sn, 0)
    # processing_pool[sn]=backend3
    # sn+=1
    # backend4 = processing_backend(sn, 0)
    # processing_pool[sn]=backend4
    # sn+=1
    # backend5 = processing_backend(sn, 0)
    # processing_pool[sn]=backend5
    # sn+=1
    # backend6 = processing_backend(sn, 0)
    # processing_pool[sn]=backend6
    # sn+=1
    # backend7 = processing_backend(sn, 0)
    # processing_pool[sn]=backend7
    # sn+=1
    # backend8 = processing_backend(sn, 0)
    # processing_pool[sn]=backend8
    # sn+=1
    # backend9 = processing_backend(sn, 0)
    # processing_pool[sn]=backend9
    # while len(threading.enumerate())>1:
    #     time.sleep(0.5)
    #     print(threading.enumerate())
    #     continue
